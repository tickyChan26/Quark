Quark

Авторские права
© 2025 tickyChan. Все права защищены.

Ограничения использования
- Этот проект предназначен только для ознакомительных и образовательных целей.
Код не лицензирован для использования, копирования, модификации, распространения или любых других действий.
Все права защищены.

Предупреждение: Любое несанкционированное использование кода является нарушением авторских прав.

----------------------

Обзор
Quark — это модель Transformer, реализованная с нуля на C++, созданная для задач обработки естественного языка (NLP) с акцентом на русский язык.
Проект не использует крупные библиотеки машинного обучения (такие как PyTorch или TensorFlow), полагаясь лишь на минимальные зависимости, например OpenBLAS для матричных операций.

----------------------

Статус проекта
- Разработка в данный момент приостановлена, обучение не проводилось из-за ограниченных аппаратных ресурсов.
Проект является каркасом. 

- Код может содержать неточности, так как находится в стадии разработки.

----------------------

Основные возможности
- Полная реализация Transformer: авторегрессионная модель Transformer, включающая Multi-Head Attention с каузальной маской, FeedForward, LayerNorm,
  позиционное кодирование и KV-кэш для эффективной генерации текста.
- Реализация на C++ с нуля: все компоненты (тензоры, градиенты, оптимизатор Adam) реализованы вручную без использования больших ML-фреймворков.
- Поддержка русского языка: интеграция с токенизатором, оптимизированным для русского текста, включая поддержку спецсимволов и смешанного ввода (русский + английский).
- Хранение данных: модель сохраняется в JSON-файлах. Поддержка других форматов (например, RocksDB) планируется в будущем.

----------------------
Архитектура
Quark имеет авторегрессионную архитектуру Transformer со следующими компонентами:

- Tensor: класс для тензорных операций, поддерживающий матричные вычисления (через OpenBLAS) и автоматическое дифференцирование для обучения.
- MultiHeadAttention: механизм многоголового внимания с каузальной маской и KV-кэшем для эффективной генерации.
- FeedForward: полносвязный слой с активацией GeLU.
- LayerNorm: слой нормализации с обучаемыми параметрами (gamma, beta).
- PositionalEncoding: синусоидальное позиционное кодирование для учёта порядка последовательностей.
- AdamOptimizer: оптимизатор Adam с клиппингом градиентов.
- TransformerModel: основной класс, объединяющий компоненты для обучения и генерации текста.
- TrainingSystem: система для загрузки данных, обучения, валидации и ранней остановки.
