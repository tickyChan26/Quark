Quark

Авторские права
© 2025 tickyChan. Все права защищены.

Ограничения использования
Этот проект предназначен только для ознакомительных и образовательных целей.
Код не лицензирован для использования, копирования, модификации, распространения или любых других действий.
Все права защищены.

Предупреждение: Любое несанкционированное использование кода является нарушением авторских прав.

----------------------

Обзор
Quark — это модель Transformer, реализованная с нуля на C++, созданная для задач обработки естественного языка (NLP) с акцентом на русский язык.
Проект не использует крупные библиотеки машинного обучения (такие как PyTorch или TensorFlow), полагаясь лишь на минимальные зависимости, например OpenBLAS для матричных операций.

----------------------

Статус проекта
Разработка в данный момент приостановлена, обучение не проводилось из-за ограниченных аппаратных ресурсов.
Проект является каркасом. Код может содержать неточности, так как находится в стадии разработки.

----------------------

Основные возможности
Полная реализация Transformer: авторегрессионная модель Transformer, включающая Multi-Head Attention с каузальной маской, FeedForward, LayerNorm, позиционное кодирование и KV-кэш для эффективной генерации текста.

Реализация на C++ с нуля: все компоненты (тензоры, градиенты, оптимизатор Adam) реализованы вручную без использования больших ML-фреймворков.

Поддержка русского языка: интеграция с токенизатором, оптимизированным для русского текста, включая поддержку спецсимволов и смешанного ввода (русский + английский).

Хранение данных: модель сохраняется в JSON-файлах. Поддержка других форматов (например, RocksDB) планируется в будущем.

----------------------
Архитектура
Quark имеет авторегрессионную архитектуру Transformer со следующими компонентами:

Tensor: класс для тензорных операций, поддерживающий матричные вычисления (через OpenBLAS) и автоматическое дифференцирование для обучения.
MultiHeadAttention: механизм многоголового внимания с каузальной маской и KV-кэшем для эффективной генерации.
FeedForward: полносвязный слой с активацией GeLU.
LayerNorm: слой нормализации с обучаемыми параметрами (gamma, beta).
PositionalEncoding: синусоидальное позиционное кодирование для учёта порядка последовательностей.
AdamOptimizer: оптимизатор Adam с клиппингом градиентов.
TransformerModel: основной класс, объединяющий компоненты для обучения и генерации текста.
TrainingSystem: система для загрузки данных, обучения, валидации и ранней остановки.
